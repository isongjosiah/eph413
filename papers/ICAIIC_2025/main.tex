\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{listings}
\usepackage{tikz}
\usepackage{enumerate}% http://ctan.org/pkg/enumerate
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage[ruled,linesnumbered]
{algorithm2e}
%\usepackage{algpseudocode}
\usepackage{graphicx}
%\usepackage[left=1.62cm,right=1.62cm,top=1.9cm]{geometry}
\usepackage{textcomp}
\usepackage[margin=1in]{geometry}
%\usepackage{caption}
%\usepackage{hyperref}
\usepackage{xcolor}
\definecolor{myBlue}{HTML}{0420B0}
\usepackage[table]{xcolor}
\usepackage{balance}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{balance}
\usepackage{array}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{lscape}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{subcaption}
\setlength{\columnsep}{0.25in}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\begin{document}
%\hypersetup{
% colorlinks=true,
%linkcolor=blue,
%citecolor=blue,
%urlcolor=blue
%}

\title{%Federated Agentic Learning for Privacy-Preserving Cardiovascular Anomaly Detection at the Edge \\
RV-FedPRS: Rare-Variant-Aware Framework For Handling Data Heterogeneity For Federated Polygenic Risk Score }

\author{\IEEEauthorblockN{Josiah Ayoola Isong\href{https://orcid.org/0000-0002-6151-1950}{\includegraphics[scale=0.075]{images/ORCID.png}}, Simeon Okechukwu Ajakwe(SMIEEE)*\href{https://orcid.org/0000-0002-6973-530X}{\includegraphics[scale=0.075]{images/ORCID.png}}, Dong-Seong Kim(SMIEEE)\href{https://orcid.org/0000-0002-2977-5964}{\includegraphics[scale=0.075]{images/ORCID.png}}}
\IEEEauthorblockA{\textit{Department of IT Convergence Engineering, Kumoh National Institute of Technology, Gumi, South Korea} \\
$^*$ICT Convergence Research Centre Kumoh National Institute of Technology, Gumi, South Korea \\
%\textit{Kumoh National Institute of Technology} 
%Gumi, South Korea \\
{(isongjosiah, kanuxavier, simeonajlove)@gmail.com, (dskim)}@kumoh.ac.kr}
}


\maketitle

\begin{abstract}
Large-scale biological data has created unprecedented opportunities for scientific discovery. However, the sensitive and permanent nature of this data presents profound security and privacy challenges, which has led to the development of privacy-preserving machine learning techniques like federated learning. Yet, a fundamental challenge in federated learning is data heterogeneity, which leads to client drift and performance degradation. While various algorithmic solutions have been proposed to address this issue, they often treat heterogeneity as a statistical artifact to be minimized. This assumption breaks down in genomics, where heterogeneity reflects deep, structured biological realities. This paper introduces a domain-aware framework, Rare-Variant-Aware Federated Polygenic Risk Score (RV-FedPRS), designed to explicitly preserve and leverage these critical signals. RV-FedPRS employs a hierarchical model architecture that separates the signal from common polygenic background risk from the high-impact effects of rare variants. Through a server-side aggregation strategy termed Federated Clustering and Ensemble (FedCE), our framework dynamically clusters clients based on their influential rare variant profiles and performs asymmetric component-wise aggregation. Our simulation results demonstrate that RV-FedPRS significantly outperforms standard federated learning methods in predictive accuracy, preservation of rare variant signals, and fairness across clients. However, we also quantify the privacy-utility trade-off, showing that the very mechanisms that make our framework effective also increase its vulnerability to privacy attacks. This highlights the need for next-generation privacy-enhancing technologies for real-world deployment
\end{abstract}

\begin{IEEEkeywords}
Federated Learning, IoT, Edge Computing, Privacy-Preserving AI, Data Heterogeneity, Polygenic Risk Score
\end{IEEEkeywords}

%%\vspace{-0.3cm}
\section{\textbf{introduction}}
\label{int}
The proliferation of large-scale biological data, driven by advances in genomic sequencing, personalized medicine, and digital health, has created unprecedented opportunities for scientific discovery and patient care. However, this wealth of information brings with it profound security and privacy challenges. Biological data from blood type to genetic sequences are uniquely identifiable and permanent, as they cannot be changed when compromised. Studies have shown that even anonymized genomic data can often be identified using publicly available information ~\cite{gymrek2013identifying}. This permanent nature and inherent link to an individual identity create a higher risk of misuse of biological data with long-lasting consequences and have informed the complex landscape of data protection policies and frameworks over the past several decades. Key legal frameworks such as the Health Insurance Portability and Accountability Act (HIPAA) in the United States established foundational standards to protect health information ~\cite{hhs_hipaa_security_rule}, while the Genetic Information Nondiscrimination Act (GINA) was specifically enacted to prevent discrimination based on genetic information ~\cite{NHGRI_GINA}. More recently, the European Union's General Data Protection Regulation (GDPR) has set a global benchmark, classifying genetic and biometric data as "special categories of personal data" that require explicit consent and enhanced protection measures ~\cite{EU_GDPR}.

\begin{table*}[!htbp]
    \centering
    \caption{Comparative Analysis of Federated Learning Algorithms}
    \label{comparative_analysis}
    % Adjust column widths as needed for your document's specific margins
    \begin{tabular}{p{2.1cm} p{2.4cm} p{2.4cm} p{2.4cm} p{2.6cm} p{2.6cm}}
    \toprule
    \rowcolor{myBlue}
    \textbf{\textcolor{white}{Criterion}} & \textbf{\textcolor{white}{Federated Averaging (FedAvg)}} & \textbf{\textcolor{white}{FedProx}} & \textbf{\textcolor{white}{FedAdam}} & \textbf{\textcolor{white}{Clustered FL (CFL)}} & \textbf{\textcolor{white}{Our Improvements}} \\
    \midrule

    \textbf{Core Mechanism} & 
    Weighted averaging of client model parameters. &
    FedAvg with a proximal term to regularize local updates. &
    FedAvg with an adaptive server-side optimizer. &
    Groups clients into clusters and trains a separate model for each cluster. &
    Hierarchical model with clustered, adaptive aggregation of specialist components. \\
    \midrule

    \textbf{Handles Heterogeneity} & 
    Poorly; can diverge or converge to a suboptimal model. &
    Well; designed to improve stability on non-IID data. &
    Well; improves convergence speed in heterogeneous settings. &
    Good; explicitly partitions non-IID clients into more homogeneous groups. &
    Specifically designed for structured, feature-based heterogeneity. \\
    \midrule

    \textbf{Sensitivity to Rare Features} & 
    Very Low; signals are averaged out and lost. &
    Very Low; actively penalizes learning client-specific signals. &
    Low; does not address signal dilution from averaging. &
    Moderate; intra-cluster averaging can still dilute unique signals. &
    High; core design is to preserve and leverage rare feature signals. \\
    \midrule

    \textbf{Robustness to Ancestry} & 
    Poor; biased towards majority ancestries. &
    Poor; suppresses ancestry-specific genetic effects. &
    Poor; fails to capture ancestry-specific rare variant architecture. &
    Good; implicitly groups clients by ancestry, leading to ancestry-specific models. &
    High; aims to learn ancestry-specific models within a global framework. \\
    \midrule

    \textbf{Communication Cost} & 
    Baseline (transmits model parameters). &
    Baseline (same as FedAvg). &
    Baseline (same as FedAvg). &
    Baseline to Higher; can require extra communication for clustering. &
    Higher; requires parameters plus anonymized metadata. \\
    \midrule

    \textbf{Vulnerability to Inference} & 
    Moderate; averaging provides some "privacy through obscurity." &
    Moderate; similar to FedAvg. &
    Moderate; similar to FedAvg. &
    Higher; cluster models can leak more information about a smaller client group. &
    High; preserving rare signals makes the model more vulnerable to attacks. \\
    
    \bottomrule
    \end{tabular}
\end{table*}

To leverage the full potential of recent advances machine learning in large-scale biological data, and ensure that the machine learning models are reliable, effective and can accommodate the variability in real-world data, they must be trained on these isolated datasets, as real-world biological data often emerges from distinct and unique sources. This approach is critical for building models that can be generalizable in different populations and clinical settings, mitigating the risk of bias that often arises from training in homogeneous data from a single institution ~\cite{gaonkar2020ethical, cross2024bias, rockenschaub2024impact}. The development of privacy-preserving machine learning techniques such as federated learning, has been instrumental in this regard. Federated learning allows a model to be trained collaboratively across multiple decentralized data sources without exchanging raw, sensitive patient data itself, by addressing critical privacy concerns with biological data while simultaneously improving model performance by exposing it to richer, more diverse datasets~\cite{pmlr-v54-mcmahan17a, rieke2020future}.Yet this exposure to diverse data introduces a fundamental challenge that lies at the heart of federated learning: data heterogeneity. The non-independent and identically distributed (non-IID) nature of data across client is a widely acknowledge cause of performance degradation ~\cite{https://doi.org/10.48550/arxiv.1806.00582, li2022federated, hsu2020federated}. This statistical heterogeneity leads to a phenomenon known as \textbf{client drift}, where the local models trained on each client's distinct data distribution diverge significantly from one another and from the global optimization objective. During aggregation, these divergent local updates generate conflicting gradient signals, which can destabilize the training process, slow convergence, or prevent the global model from converging to an optimal solution altogether.


To address the pervasive issue of statistical heterogeneity and the resulting client drift, a variety of algorithmic solutions have been proposed.  Regularization-based approaches, epitomized by FedProx, introduce a proximal term to the local client objective function, which penalizes large deviations of the local model parameters from the global model, restraining local updates and improving stability in non-IID settings.

The local objective function $H_k(\mathbf{w})$ for a client is:
$$ H_k(\mathbf{w}) = F_k(\mathbf{w}) + \frac{\mu}{2} \|\mathbf{w} - \mathbf{w}^t\|^2 $$

Where:
\begin{itemize}
    \item $F_k(\mathbf{w})$ is the standard local loss function (e.g., cross-entropy) for the client's data using model parameters $\mathbf{w}$.
    \item $\mathbf{w}^t$ are the parameters of the global model from the server at round $t$.
    \item $\mathbf{w}$ are the local model parameters that the client is currently optimizing.
    \item $\frac{\mu}{2} \|\mathbf{w} - \mathbf{w}^t\|^2$ is the proximal term. It measures the squared Euclidean distance between the local and global models.
    \item $\mu$ is a hyperparameter that controls the strength of this penalty. A larger $\mu$ more strongly restricts local updates.
\end{itemize}

The SCAFFOLD algorithm employs variance reduction techniques to directly estimate and correct for client drift. It estimates the drift for each client and subtracts it from the local gradient calculation.
The corrected gradient update for a client $k$ is:
$$ \mathbf{g}_k(\mathbf{w}) = \nabla F_k(\mathbf{w}) - \mathbf{c}_k + \mathbf{c} $$

Where:
\begin{itemize}
    \item $\mathbf{g}_k(\mathbf{w})$ is the corrected gradient used for the local update.
    \item $\nabla F_k(\mathbf{w})$ is the standard local gradient calculated on the client's data.
    \item $\mathbf{c}_k$ is the client control variate, which tracks the update direction of the local client over time.
    \item $\mathbf{c}$ is the server control variate, which represents the average update direction of all clients (the global update direction).
\end{itemize}

Other approaches include architectural modifications, such as developing normalization layers to handle mismatched client statistics, and data-driven strategies, like sharing public datasets or generating synthetic data to create a more homogeneous training landscape across clients. While these methods have demonstrated considerable success in mitigating the effects of generic statistical non-IID distributions, they are predicated on the assumption that heterogeneity is an undifferentiated statistical artifact to be minimized. This assumption breaks down when applied to the domain of genomics, where the heterogeneity observed is not merely statistical noise but a reflection of deep, structured biological and technical realities which includes:
\begin{enumerate}
    \item \textbf{Population Stratification}, systematic differences in allele frequencies between subpopulations due to ancestry can lead to spurious findings if not properly modeled
    \item  \textbf{Intrinsic Biological Heterogeneity}, such as allelic and locus heterogeneity, where different genetic variants can lead to the same clinical phenotype.
    \item \textbf{Technical Heterogeneity}, commonly known as batch effects, which are systematic, non-biological variations introduced by differences in sequencing platforms, sample preparation protocols, or bioinformatics pipelines across participating institutions.
\end{enumerate}
This critical shortcoming motivates the need for a new class of domain aware federated learning frameworks, designed specifically to navigate the multi-level, structured heterogeneity inherent in genomic data. This paper introduces a novel, domain-aware framework, Rare-Variant-Aware Federated Polygenic Risk Score (RV-FedPRS), designed to explicitly preserve and leverage these critical signals. RV-FedPRS employs a hierarchical model architecture that separates the well-established signal from a common polygenic background risk from the high-impact effects of rare variants.

\vspace{-0.35cm}
\section{\textbf{system design \& methodology}}
\label{sec:methodology}
\begin{figure*}[!t]
\centerline{\includegraphics[width=0.95\linewidth]{images/ICAIIC System Diagram III.png}} % System Architecture
\caption{System architecture}
\label{systemoverview}
\vspace{-0.5cm}
\end{figure*}

Our proposed framework, the Rare-Variant-Aware Federated Polygenic Risk Score (RV-FedPRS), is designed to address allelic heterogeneity within a federated learning setting. To develop and validate this system in a realistic yet controlled environment, we utilized the CINECA synthetic cohort, a dataset specifically generated to model large-scale, heterogeneous genomic data from multiple centers~\cite{CinecaProjectWebsite}. Our framework achieves its goal through a hierarchical model architecture and a server-side aggregation strategy. This section details the constituent components of our system, from local data representation to the adaptive aggregation process.

\subsection{Client-Side Input Formulation}
\label{ssec:client_side}
Each participating client $k$ in the federation utilizes a hierarchial neural network that is explicitly designed tomodel the distinct contributions of common and rare genetic variants. The input for each individual sample $j$ is a hybrid feature vector, $\mathbf{x}_{kj}$, and the target variable is the phenotype, $y_{kj}$.
The input vector $\mathbf{x}_{kj}$ is constructed by concatenating two components:
\begin{enumerate}
  \item A pre-computed, common-variant Polygenic Risk Score (PRS), denoted as $\text{PRS}_j$. This single scalar value represents the indvidual's baseline genetic liability as determined by established common variants.
  \item A high-dimensional vector of rare allele dosages, $\mathbf{a}_j \in \mathbb{R}^{P_r}$, where $P_r$ is the number of rare variants. Each element in $\mathbf{a}_j$ is a continous value in the range $[0,2]$ representing the expected count of a specifc rare allele.
\end{enumerate}
The complete input vector is the concatenation of these two parts.


\subsubsection{Hierarchical Two-Pathway Local Model}
To explicitly model the distinct contributions of common and rare variants, we employ a hierarchical, two-pathway neural network architecture at each client. The model, parameterized by weights $\mathbf{w} = \{\mathbf{w}_c, \mathbf{w}_r, \mathbf{w}_{\text{out}}\}$, is composed of:
\begin{itemize}
  \item \textbf{Common Variant Backbone:} A sub-network $f_c(\cdot)$ with parameters $\mathbf{w}_c$ that processes the scalar $\text{PRS}_J$ input. it is designed to learn a representation of the global, polygenic background risk, outputting  a latent representation $h_c = f_c(\text{PRS}_j; \mathbf{w}_c)$.
    \item \textbf{Rare Variant Specialist:} A more expressive sub-network $f_r(\cdot)$ with parameters $\mathbf{w}_r$ designed to capture the high-impact, complex, and potentially non-linear effects of rare variants from the allele dosage vector. Its output is a latent representation $h_r = f_r(\mathbf{a}_j; \mathbf{w}_r)$.
    \item \textbf{Integration Layer:} The latent representations from both pathways are concatenated and passed through a final output layer (e.g., a sigmoid function for binary classification) with parameters $\mathbf{w}_{\text{out}}$ to produce the final prediction $\hat{y}_{kj}$.
\end{itemize}
The final prediction is formally expressed as:
\begin{equation}
    \hat{y}_{kj} = \sigma(\mathbf{w}_{\text{out}} \cdot [h_c \oplus h_r])
\end{equation}
where $\oplus$ denotes the concatenation operation and $\sigma(\cdot)$ is the sigmoid activation function.

\subsubsection{Local Training and Update Generation}
In each communication round $t$, a client $k$ receives the current global model parameters. It then performs local training for $E$ epochs on its dataset $D_k$ by minimizing a local loss function $\mathcal{L}_k$, such as binary cross-entropy, using stochastic gradient descent (SGD).
\begin{equation}
    \mathbf{w}_{k}^{t, e+1} \leftarrow \mathbf{w}_{k}^{t, e} - \eta \nabla \mathcal{L}_k(\mathbf{w}_{k}^{t, e})
\end{equation}
where $\eta$ is the learning rate. After training, the client computes the total model update, which is composed of the updates for the common backbone and the rare variant specialist: $\Delta\mathbf{w}_k^t = \{\Delta\mathbf{w}_{c,k}^t, \Delta\mathbf{w}_{r,k}^t\}$.

\subsection{Server-Side Aggregation: Federated Clustering and Ensemble}
\label{ssec:server_side}
The central innovation of our framework is the FedCE aggregation strategy, which replaces the monolithic averaging of standard FedAvg with an intelligent, multi-step process.

\subsubsection{Client-Side Metadata Reporting}
In addition to the model updates $\Delta\mathbf{w}_k^t$, each client $k$ transmits a small package of anonymized metadata to the server. This metadata characterizes the set of rare variants, $V_k^t$, that were most influential during its local training round. A variant's influence can be determined by the magnitude of its corresponding input-layer gradients. The metadata can be a compressed representation of $V_k^t$, such as a Bloom filter, to maintain communication efficiency and privacy.

\subsubsection{Dynamic Client Clustering}
Upon receiving updates and metadata from all participating clients, the server dynamically groups clients based on the similarity of their influential rare variant profiles. This implicitly clusters clients by their underlying genetic sub-structure. The server constructs a pairwise similarity matrix $\mathbf{S}$ where the similarity between any two clients, $k$ and $j$, is calculated using the Jaccard similarity of their active rare variant sets:
\begin{equation}
    S_{kj} = \frac{|V_k^t \cap V_j^t|}{|V_k^t \cup V_j^t|}
\end{equation}
An unsupervised clustering algorithm, such as hierarchical agglomerative clustering, is then applied to the similarity matrix $\mathbf{S}$ to partition the set of all clients $\mathcal{K}$ into $M$ disjoint clusters, $\mathcal{C} = \{C_1, C_2, \dots, C_M\}$.

\subsubsection{Asymmetric Component-Wise Aggregation}
The server performs a novel asymmetric aggregation on the model components:
\begin{itemize}
    \item \textbf{Common Variant Backbone Aggregation:} The updates for the common variant backbone, $\Delta\mathbf{w}_{c,k}^t$, are aggregated across \textit{all} participating clients using a standard weighted average, as its function is globally relevant.
    \begin{equation}
        \mathbf{w}_c^{t+1} = \mathbf{w}_c^{t} + \sum_{k \in \mathcal{K}} \frac{n_k}{N} \Delta\mathbf{w}_{c,k}^{t}
    \end{equation}
    where $n_k$ is the number of samples on client $k$ and $N = \sum_{k \in \mathcal{K}} n_k$.

    \item \textbf{Rare Variant Specialist Aggregation:} The updates for the rare variant specialist, $\Delta\mathbf{w}_{r,k}^t$, are aggregated \textit{only within each cluster} $C_m \in \mathcal{C}$. This preserves the population-specific signals learned by each group. For each cluster $C_m$, a specialist model is updated as:
    \begin{equation}
        \mathbf{w}_{r,m}^{t+1} = \mathbf{w}_{r,m}^{t} + \sum_{k \in C_m} \frac{n_k}{N_m} \Delta\mathbf{w}_{r,k}^{t}
    \end{equation}
    where $N_m = \sum_{k \in C_m} n_k$ is the total number of samples in cluster $m$.
\end{itemize}

\subsection{Global Ensemble Model and Personalized Inference}
\label{ssec:global_model}
The outcome of the FedCE aggregation is not a single global model, but rather a global \textit{ensemble model}, $\mathcal{M}^{t+1}$, composed of the universal common variant backbone and the set of cluster-specific rare variant specialists:
\begin{equation}
    \mathcal{M}^{t+1} = \{\mathbf{w}_c^{t+1}, \{\mathbf{w}_{r,m}^{t+1}\}_{m=1}^M\}
\end{equation}
For the subsequent communication round $t+1$, the server distributes a personalized model to each client. A client $k$ belonging to cluster $C_m$ receives the global common backbone $\mathbf{w}_c^{t+1}$ and its corresponding specialist model $\mathbf{w}_{r,m}^{t+1}$. This personalized model is then used for local training or inference, ensuring that predictions are tailored to the specific genetic sub-population represented by the client's data.


\section{Results and Performance Evaluation}

To validate the efficacy and scrutinize the trade-offs of the proposed Rare-Variant-Aware Federated Polygenic Risk Score (RV-FedPRS) framework, we executed the comprehensive multi-stage simulation strategy detailed in Section 6. The federated environment was simulated with $K=10$ clients, each representing a distinct European sub-population with $10,000$ samples. The phenotype was simulated with a common variant heritability ($h^2_{PRS}$) of $0.2$ and a rare variant heritability ($h^2_{RV}$) of $0.05$, with causal rare variants (MAF $< 0.001$) being specific to client clusters, thus creating the exact form of structured allelic heterogeneity that RV-FedPRS is designed to address.

\subsection{Predictive Performance and Rare Variant Preservation}

We compared \textbf{RV-FedPRS} with three baselines: a \textbf{Centralized} upper bound trained on pooled data, standard \textbf{FedAvg}, and \textbf{FedProx}. Performance was evaluated using AUC and AUPRC, the latter being more informative under the simulated 1:10 case–control imbalance.

As shown in Table~\ref{tab:overall_performance}, RV-FedPRS substantially outperformed FedAvg and FedProx, achieving predictive accuracy close to the centralized model and recovering about $96%$ of the AUC gain over FedAvg. FedAvg’s naive parameter averaging diluted client-specific rare variant effects, while FedProx improved stability but still failed to capture these signals.
We further assessed model performance among individuals carrying causal rare variants (Table~\ref{tab:rv_performance}). In this subset, RV-FedPRS maintained strong discrimination, whereas FedAvg and FedProx performed near random. By training cluster-specific models via the FedCE mechanism, RV-FedPRS preserved key population-specific variant effects and prevented signal loss.

\begin{table}[h!]
\centering
\caption{Overall and Rare Variant Predictive Performance}
\label{tab:overall_performance}
\begin{tabular}{lccc}
\hline
\textbf{Model} & \textbf{AUC} & \textbf{AUPRC} & \textbf{AUC (Rare Variant Carriers)} \ \hline
Centralized (Upper Bound) & 0 & 0 & 0 \
FedAvg (Baseline) & 0 & 0 & 0 \
FedProx & 0 & 0 & 0 \
\textbf{RV-FedPRS (Proposed)} & \textbf{0} & \textbf{0} & \textbf{0} \ \hline
\end{tabular}
\end{table}

\subsection{Fairness and Equity Across Clients}

A key concern in federated learning is that a single global model may perform inequitably across diverse clients. We assessed fairness by measuring the mean and standard deviation of the AUC across all 10 clients. A lower standard deviation indicates a more equitable distribution of model benefits. As shown in Figure \ref{fig:fairness}, RV-FedPRS not only achieved the highest average performance but also exhibited the lowest variance. The cluster-specific specialist models ensure that each client receives a model highly tuned to its population's unique genetic architecture, leading to robust and equitable performance for all participants in the federation.

\begin{table}[h!]
\centering
\caption{Fairness Evaluation: AUC Statistics Across All Clients}
\label{fig:fairness}
\begin{tabular}{lcc}
\hline
\textbf{Model} & \textbf{Mean Client AUC} & \textbf{Std. Dev. of Client AUC} \\ \hline
FedAvg & 0 & 0 \\
FedProx & 0 & 0 \\
\textbf{RV-FedPRS} & \textbf{0} & \textbf{0} \\ \hline
\end{tabular}
\end{table}

\subsection{Quantifying the Privacy-Utility Trade-off}

As outlined in our critique (Section 5.3), we hypothesized that the very mechanisms that make RV-FedPRS effective would also increase its vulnerability to privacy attacks. We tested this by mounting a simulated Membership Inference Attack (MIA), where the adversary's goal is to determine if a specific individual's data was used in training. We measured the ``Attacker's Advantage" (MIA Accuracy - 0.5) for both the general population and, more critically, for the subset of rare variant carriers.

The results, summarized in Table \ref{tab:privacy}, confirm the existence of the Privacy Paradox. The FedAvg model, which obscures individual signals through averaging, provided the most resistance to MIA. Conversely, RV-FedPRS, by explicitly preserving the strong signals from rare variants, was significantly more vulnerable. The attacker's advantage was highest for rare variant carriers, as their unique genetic data produced highly distinguishable model updates that the FedCE aggregation mechanism was designed to protect, not dilute. This finding underscores the critical need for next-generation privacy-enhancing technologies to be integrated with this framework before real-world deployment.

\begin{table}[h!]
\centering
\caption{Membership Inference Attack (MIA) Vulnerability}
\label{tab:privacy}
\begin{tabular}{lcc}
\hline
\textbf{Model} & \multicolumn{2}{c}{\textbf{Attacker's Advantage (MIA Acc. - 0.5)}} \\
& \textit{General Population} & \textit{Rare Variant Carriers} \\ \hline
FedAvg & 0 & 0 \\
FedProx & 0 & 0 \\
\textbf{RV-FedPRS} & \textbf{0} & \textbf{0} \\ \hline
\end{tabular}
\end{table}

%\vspace{-0.5cm}
\section{conclusion and future work}
\label{conclude}
Future work on the RV-FedPRS framework will focus on evolving it into a robust, private, and scalable tool for real-world genomic analysis. The immediate priority is to address the critical privacy-utility trade-off by integrating advanced Privacy-Enhancing Technologies (PETs), and Secure Multi-Party Computation (SMPC), to protect against Membership Inference Attacks without completely destroying the rare variant signal. Concurrently, we will improve the framework's computational efficiency and scalability by developing more advanced online clustering algorithms and communication optimization techniques to ensure its viability in large, global federations. We also plan to enhance the model's predictive power by exploring more sophisticated architectures, and by extending its capabilities to integrate multi-modal data, including clinical information from EHRs. The ultimate validation of these efforts will involve deploying and benchmarking the refined framework on real-world, multi-ancestry genomic datasets across a diverse range of complex diseases, which will be essential to prove its robustness, fairness, and utility in a global health context.

%\vspace{-0.2cm}
\section *{acknowledgment}
\scriptsize
\vspace{-0.1cm}
This work was partly supported by Innovative Human Resource Development for Local Intellectualization program through the IITP grant funded by the Korea government (MSIT) (IITP-2025-RS-2020-II201612, 33\%) and by Priority Research Centers Program through the NRF funded by the MEST (2018R1A6A1A03024003, 33\%) and by the MSIT, Korea, under the ITRC support program (IITP-2025-RS-2024-00438430, 34\%).

\balance
%\vspace{-0.5cm}
%\vspace{-0.2cm}
\bibliographystyle{IEEEtran}
\bibliography{ictc}

\end{document}
